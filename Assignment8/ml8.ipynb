{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f49e01ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "57/57 [==============================] - 0s 892us/step\n",
      "RMSE 0.227387684609182\n",
      "32/32 [==============================] - 0s 867us/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dot, Reshape, Add, Subtract\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_path = \"C:/Users/lenovo/Desktop/机器学习/Assignment8/\"\n",
    "trval = pd.read_csv(data_path + 'train_sample.csv')\n",
    "test = pd.read_csv(data_path + 'test_sample.csv')\n",
    "feats = [f\"feature_{i}\" for i in range(0, 3)]\n",
    "\n",
    "x_trval, y_trval = trval[feats], trval[\"target\"]\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_trval, y_trval, test_size=0.20, random_state=4, stratify=y_trval)\n",
    "\n",
    "\n",
    "x_train = [x_train[f].values for f in feats]\n",
    "x_val = [x_val[f].values for f in feats]\n",
    "x_test = [test[f].values for f in feats]\n",
    "\n",
    "y_train = y_train.values\n",
    "y_val = y_val.values\n",
    "\n",
    "\n",
    "def get_embed(x_input, x_size, out_dim, embedding_reg=0.0002):\n",
    "    # x_input is index of input (either user or item)\n",
    "    # x_size is length of vocabulary (e.g. total number of users or items)\n",
    "    # out_dim is size of embedding vectors\n",
    "    if x_size > 0: #category\n",
    "        embed = Embedding(x_size, out_dim, input_length=1, embeddings_regularizer=l2(embedding_reg))(x_input)\n",
    "        embed = Flatten()(embed)\n",
    "    else:\n",
    "        embed = Dense(out_dim, kernel_regularizer=l2(embedding_reg))(x_input)\n",
    "    return embed\n",
    "\n",
    "\n",
    "def build_model(f_size, k_latent=2, kernel_reg=0.05):\n",
    "    dim_input = len(f_size)\n",
    "    input_x = [Input(shape=(1, )) for i in range(dim_input)] \n",
    "    lin_terms = [get_embed(x, size, 1) for (x, size) in zip(input_x, f_size)]\n",
    "    factors = [get_embed(x, size, k_latent) for (x, size) in zip(input_x, f_size)]\n",
    "    s = Add()(factors)\n",
    "    diffs = [Subtract()([s, x]) for x in factors]\n",
    "    dots = [Dot(axes=1)([d, x]) for d, x in zip(diffs, factors)]\n",
    "    x = Concatenate()(lin_terms + dots)\n",
    "    x = BatchNormalization()(x)\n",
    "    output = Dense(1, activation='relu', kernel_regularizer=l2(kernel_reg))(x)\n",
    "    model = Model(inputs=input_x, outputs=[output])\n",
    "    model.compile(optimizer=Adam(clipnorm=0.6, learning_rate=0.001), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "\n",
    "n_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "f_size = [int(x_trval[f].max()) + 1 for f in feats]\n",
    "model = build_model(f_size)\n",
    "earlystopper = EarlyStopping(patience=20, verbose=0, restore_best_weights=True)\n",
    "model.fit(x_train, y_train, epochs=n_epochs, batch_size=batch_size, verbose=0, validation_data=(x_val, y_val), callbacks=[earlystopper])\n",
    "\n",
    "p = np.squeeze(model.predict(x_val))\n",
    "print(\"RMSE\", mean_squared_error(y_val, p) ** 0.5)\n",
    "\n",
    "preds_test = np.round(np.squeeze(model.predict(x_test))).astype(int)\n",
    "test['target'] = preds_test\n",
    "test.to_csv('submission.csv', index=False, columns=[\"ID\", \"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1e172a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7fc8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
